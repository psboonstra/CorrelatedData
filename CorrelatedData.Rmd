---
title: "Regression methods for correlated outcomes: a primer"
author: "Phil Boonstra, borrowing from lots of previous 699 notes"
date: "Univeristy of Michigan Biostatistics 699, Winter 2022"
#geometry: margin=0.5in # uncomment this for making pdfs
output: 
  ioslides_presentation:
    transition: 0.1
    css: ~/Desktop/Work/CV/Styles/temp_fancy_logo_only_first.css
    includes:
      in_header: ~/Desktop/Work/CV/Styles/slides_header.html
    incremental: no
    widescreen: true
    logo: ~/Desktop/Work/CV/Styles/Biostat-informal.png
    slide_level: 2
  beamer_presentation:
    highlight: zenburn
    includes:
      in_header: ~/Desktop/Work/CV/Styles/tex_header.txt
    keep_tex: yes
    theme: Pittsburgh
    toc: no
    fig_caption: false
  pdf_document:
    highlight: zenburn
    includes:
      in_header: ~/Desktop/Work/CV/Styles/tex_pdf_header.txt
    keep_tex: yes
    toc: no
    fig_caption: false
    fig_height: 6
    fig_width: 3
---

```{r setup, include=FALSE}
library(tidyverse); library(broom);
library(knitr); library(ggplot2)
knitr::opts_chunk$set(echo = T, warning = F, message = F);
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size);
})

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})

#knitr::opts_chunk$set(width = 10);
#knitr::opts_chunk$set(tidy.opts=list(width.cutoff=40));
recache = F;
options(digits = 4);
figure_scaler = 1/2; #1/2 for ioslides; ~1/3 for word, pdf
text_scaler = 3/3;#1 for ioslides; 2/3 for word, pdf
fig.x = 16 * figure_scaler;
fig.y = 9 * figure_scaler;
```

##  Linear model

$$\mathbf{Y} = \mathbf{x} \mathbf{\beta} + \mathbf{\varepsilon}$$


- $\mathbf{Y}$, $\mathbf{\varepsilon}$ are $n\times 1$
- $\mathbf{x}$ is $n\times p$ (assumed fixed and known, so $\mathrm{var}(\mathbf{Y}) = \mathrm{var}(\mathbf{\varepsilon})$)
- $\mathbf{\beta}$ is $p \times 1$

## Mean structure

If $E[\mathbf{\varepsilon}] = \mathbf{0}$, then $E[\mathbf{Y}] = \mathbf{x} \mathbf{\beta}$, and 

$$\hat{\mathbf{\beta}} \overset{set}{=} (\mathbf{x}^\top \mathbf{x})^{-1}\mathbf{x}^\top \mathbf{Y}$$ is 
unbiased for $\mathbf{\beta}$, i.e. $E[\hat{\mathbf{\beta}}] = \mathbf{\beta}$

When $\mathrm{var}(\varepsilon_i) = \sigma^2$ $\forall i$ and 
$\mathrm{cov}(\varepsilon_i, \varepsilon_{i'})=0$ for $i\neq i'$, then $\hat\beta$ 
is best linear unbiased estimator. Best = smallest mean squared error

## Distributional structure

$\mathbf{\varepsilon} \sim \mathrm{MVN}(\mathbf{0}, \sigma^2 \mathbf{I})$, 

$$ \mathbf{I} = 
\begin{pmatrix}
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots &\vdots &\ddots &\vdots \\
0 & \cdots & 0 & 1
\end{pmatrix}
$$

## Generalizing linear model

- More general mean structure, e.g. $E[\mathbf{Y}] = g(\mathbf{x} \mathbf{\beta})$ for link
function $g$ ("generalized linear model")

- Relax normality assumption on $\mathbf{\varepsilon}$, e.g. $t$-distributed ("robust regression")

- Allow $\mathrm{Var}(\mathbf{\varepsilon}) = \sigma^2\mathbf{D}$, where $\mathbf{D}$ is diagonal matrix with positive elements ("heteroscedasticity")

- Allow $\mathrm{Var}(\mathbf{\varepsilon}) = \mathbf{\Sigma}$, where $\mathbf{\Sigma}$ is positive definite (correlated errors)

## Question 

Why not just always make most flexible distributional assumption $\mathrm{Var}(\mathbf{\varepsilon}) = \mathbf{\Sigma}$?

<!-- Estimating more parameters, leading to loss of efficiency and risk of overfitting-->

<!-- Usually assume structured form for $\mathbf{\Sigma}$ -->

## Question

When standard assumptions for linear models hold, $(\mathbf{x}^\top \mathbf{x})^{-1}\mathbf{x}^\top \mathbf{Y}$ is unbiased estimator of $\mathbf{\beta}$

Is $(\mathbf{x}^\top \mathbf{x})^{-1}\mathbf{x}^\top \mathbf{Y}$ still unbiased when $\mathrm{Var}(\mathbf{\varepsilon}) \neq \sigma^2 \mathbf{I}$?

<!-- yes if linearity is satisfied and if errors are conditionally mean-zero given $\mathbf{x}$ (exogeneity)-->

## Compound symmetric

Constant correlation between all observations

$$\mathbf{\Sigma}_{cs} =
\sigma^2 \begin{pmatrix}
1 & \rho & \cdots & \rho\\
\rho & 1 & \cdots &  \rho\\
\vdots &\vdots &\ddots &\vdots \\
\rho & \cdots &  \rho & 1
\end{pmatrix}
$$

## Auto-regressive 

Distance-based correlation

- assumes observations are "equally spaced" and ordered

$$\mathbf{\Sigma}_{ar} =
\sigma^2 \begin{pmatrix}
1 & \rho & \rho^2 & \cdots & \rho^{n-1}\\
\rho & 1 & \rho & \cdots &  \rho^{n-2}\\
\rho^2 & \rho & 1 & \cdots &  \rho^{n-3}\\
\vdots &\vdots &\vdots & \ddots &\vdots \\
\rho^{n-1} & \rho^{n-2} & \cdots &  \rho & 1
\end{pmatrix}
$$

## Block diagonal 

Grouped correlation

- depends on external knowledge about grouping

$$\mathbf{\Sigma}_{bd} =
\begin{pmatrix}
\mathbf{\Gamma} & 0 & \cdots & 0\\
0 & \mathbf{\Gamma} & \cdots & 0\\
\vdots &\vdots &\ddots &\vdots \\
0 & \cdots & 0 & \mathbf{\Gamma}
\end{pmatrix}
$$
$\mathbf{\Gamma}$ can be compound symmetric, auto-regressive, etc

## Example with equally sized groups

Let $i = 1,\ldots,n$ denote groups; $j = 1,\ldots,m$ denote individuals within group; total of $n\times m$ observations. Model could be:

$$Y_{ij} = \mathbf{x}_{ij}^\top \mathbf{\beta} + \varepsilon_{ij}$$

Would expect $\varepsilon_{ij}$ and $\varepsilon_{i{j'}}$ (same $i$, different $j$) to be correlated, but $\varepsilon_{ij}$ and $\varepsilon_{{i'}j}$ (different $i$) are likely uncorrelated

## Example with equally sized groups


Write $\varepsilon_{ij} = b_i + \zeta_{ij}$, where $b_i$ is shared group effect and $\zeta_{ij}$ is error specific to each observation:

$$Y_{ij} = \mathbf{x}_{ij}^\top \mathbf{\beta} +  b_i + \zeta_{ij}$$

- Observations in same group, i.e. same $i$, have same $b_i$ and so will be more alike than observations in different groups. Based on this, more reasonable to expect that $\zeta_{ij}$ are independent for any $i$, $j$.

- Group effects are viewed as random and not parameters, "random effects" 

## Example with equally sized groups

Typical to assume
$b_i\overset{iid}{\sim} N(0, \tau^2)$ and $e_{ij} \overset{iid}{\sim} N(0,\phi^2)$. 

$$
\begin{aligned}
\mathrm{Var}(Y_{ij}) = \mathrm{Var}(b_i + \zeta_{ij}) &= \tau^2+\phi^2\\
\mathrm{Cov}(Y_{ij}, Y_{i{j'}})  = \mathrm{Cov}(b_i + \zeta_{ij}, b_i + \zeta_{i{j'}}) &= \tau^2\\
\mathrm{Cov}(Y_{ij}, Y_{{i'}j})  = \mathrm{Cov}(b_i + \zeta_{ij}, b_{i'} + \zeta_{{i'}j})  &= 0 
\end{aligned}
$$

This corresponds to block diagonal with 

$$
\mathbf{\Gamma} =
\begin{pmatrix}
\tau^2+\phi^2 & \tau^2 & \cdots & \tau^2\\
\tau^2 & \tau^2+\phi^2 & \cdots & \tau^2\\
\vdots &\vdots &\ddots &\vdots \\
\tau^2 & \cdots & \tau^2 & \tau^2+\phi^2
\end{pmatrix}
$$



## Conditional vs. marginal models

### Conditional model

$$Y_{ij} = \mathbf{x}_{ij}^\top \mathbf{\beta}  + b_i + \zeta_{ij}$$
$\mathrm{Var}(b_i) = \tau^2$; $\mathrm{Var}(\zeta_{ij}) = \phi^2$; everything independent


### Marginal model

$$Y_{ij} = \mathbf{x}_{ij}^\top \mathbf{\beta}  + \varepsilon_{ij}$$

If we assume compound symmetric correlation structure on $\mathbf{\varepsilon}$, i.e. 
$\mathrm{Var}(\varepsilon_{ij}) = \sigma^2$ and $\mathrm{Cov}(\varepsilon_{i1}, \varepsilon_{i2}) = \rho\sigma^2$, then these coincide by observing

$$
\begin{aligned}
\rho = \tau^2 / (\tau^2 + \phi^2)\\
\sigma^2 = \tau^2 + \phi^2
\end{aligned}
$$

## General form of conditional linear model

$$
\begin{aligned}
\mathbf{Y}_i &= \mathbf{x}_i^\top \mathbf{\beta}  + \mathbf{z}_i^\top\mathbf{b}_i+ \mathbf{\zeta}_i\\
\mathbf{b}_i &\overset{iid}{\sim} N(\mathbf{0}, \mathbf{G})\\
\mathbf{\zeta}_i &\overset{iid}{\sim} N(\mathbf{0}, \sigma^2 \mathbf{I})\\
&\Rightarrow \mathrm{Var}(\mathbf{Y}_i) = \mathbf{z}_i \mathbf{G} \mathbf{z}_i^\top + \sigma^2 \mathbf{I}
\end{aligned}
$$

- $x$ and $z$ can be distinct, completely overlapping, or somewhere between
- `R` functions `nlme::nlme`, `lme4::lmer` (normal-linear models), `lme4::glmer` (generalized linear models)

\scriptsize
I'm switching notation here: replacing $\phi$ with $\sigma$ 

## Several names for conditional model

- Called "conditional" because of perspective that $b_i$ is random, and outcomes $Y_i$ are only conditionally independent given $b_i$

- Called "mixed effects" because mix of fixed ($\mathbf{\beta}$) and random ($\mathbf{b}_i$) effects

- Called "hierarchical" or "multilevel" because sometimes there is clear 
hierarchy in data


## General form of marginal model

$$
\begin{aligned}
\mathbf{Y}_i &= \mathbf{x}_i^\top \mathbf{\beta}  + \mathbf{\varepsilon}_i\\
\mathbf{\varepsilon}_i & \overset{ind}{\sim} N(\mathbf{0}, \mathbf{R}_i)
\end{aligned}
$$

- Called "marginal" because random effects have been integrated or marginalized out
- `R` function `nlme::gls` (generalized least squares), `gee::gee` or 
`geepack::geeglm` (generalized estimating equations [GEE])

## Summary points so far

- Both approaches trying to do inference on $\mathbf{\beta}$ accounting for correlation
between outcomes

- There is 1-1 correspondence between conditional linear model with single random
intercept and marginal model with compound symmetric correlation structure on errors


## Robust variance estimator

Suppose we have matrix $\mathbf{W}$ that is assumed covariance for $\mathbf{Y}$, then estimate of $\mathbf{\beta}$ given by 

$$
\hat{\mathbf{\beta}}= (\mathbf{x}^\top \mathbf{W}^{-1}\mathbf{x})^{-1} \mathbf{x}^\top \mathbf{W}^{-1} \mathbf{Y}
$$
and variance expression given by 

$$
\begin{aligned}
\mathrm{Var}(\hat{\mathbf{\beta}}) &= (\mathbf{x}^\top \mathbf{W}^{-1} \mathbf{x})^{-1}\times \\
&\quad(\mathbf{x}^\top \mathbf{W}^{-1} \mathbf{\Sigma} \mathbf{W}^{-1} \mathbf{x})\times\\
&\quad(\mathbf{x}^\top \mathbf{W}^{-1} \mathbf{x})^{-1}
\end{aligned}
$$
where $\mathbf{\Sigma}$ is true, unknown covariance

## Robust variance estimator

Suggests recipe for potentially better variance estimate of $\hat{\mathbf{\beta}}$:

1. Choose structure for $\mathbf{W}$, e.g. $\mathbf{W} = \sigma^2 \mathbf{I}$ or something fancier 

2. Calculate $\hat{\mathbf{\beta}}= (\mathbf{x}^\top \hat{\mathbf{W}}^{-1}\mathbf{x})^{-1} \mathbf{x}^\top \hat{\mathbf{W}}^{-1} \mathbf{Y}$

3. Calculate $\hat{\mathbf{\Sigma}} = (1/n) {\hat{\mathbf{e}}}^\top \hat{\mathbf{e}}$ (or different estimate)

4. Calculate 

$$
\begin{aligned}
\hat{\mathrm{Var}}(\hat{\mathbf{\beta}}) &= (\mathbf{x}^\top \hat{\mathbf{W}}^{-1} \mathbf{x})^{-1}\times \\
&\quad\mathbf{x}^\top \hat{\mathbf{W}}^{-1} \hat{\mathbf{\Sigma}} \hat{\mathbf{W}}^{-1} \mathbf{x}\times\\
&\quad(\mathbf{x}^\top \hat{\mathbf{W}}^{-1} \mathbf{x})^{-1}
\end{aligned}
$$

<!-- Robust variance estimator uses residuals from model fit to estimate $\mathbf{\Sigma}$ -->


## Sleepstudy example

```{r, echo = TRUE, include = TRUE, eval = FALSE}
library(lme4)
?sleepstudy
```


> The average reaction time per day for subjects in a sleep deprivation study. 
On day 0 the subjects had their normal amount of sleep. Starting that night 
they were restricted to 3 hours of sleep per night. The observations represent 
the average reaction time on a series of tests given each day to each subject.

```{r, echo = FALSE}
library(lme4);
```

##

> **Format**
A data frame with 180 observations on the following 3 variables.

> **Reaction**
Average reaction time (ms)

> **Days**
Number of days of sleep deprivation

> **Subject**
Subject number on which the observation was made.


## Observed data

```{r, echo = F, cache = F, size = "scriptsize", fig.width = fig.x, fig.height=fig.y}
ggplot(sleepstudy, aes(x = Days, y = Reaction, group = Subject)) + 
  geom_line() + 
  scale_x_continuous(breaks = 0:9, minor_breaks = NULL) + 
  theme(text = element_text(size = 22));
```

## 

#### Conditional model using `lme4::lmer`, random intercept

```{r, cache = F}
cm_sleep <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
```


#### Marginal model using `lm` (OLS), assumes independent errors

```{r, cache = F}
mm_sleep_ind <- lm(Reaction ~ Days, sleepstudy)
```

#### Marginal model using `nlme::gls`, compound symmetric error structure

```{r, cache = F}
library(nlme)
mm_sleep_cs <- gls(Reaction ~ Days, sleepstudy, 
                   correlation = corCompSymm(form = ~1|Subject))
```


#### Marginal model using `geepack::geeglm`, CS error structure + robust variance

```{r, cache = F}
library(geepack)
mm_sleep_gee_cs <- 
  geeglm(Reaction ~ Days, 
         data = sleepstudy, 
         id = Subject,
         corstr = "exchangeable")
```


#### Marginal model using `gee::gee`, same as above

```{r, cache = F, message = F, results = F}
library(gee)
mm_sleep_gee_cs_alt <-
  gee(Reaction ~ Days, data = sleepstudy, id = Subject,
      corstr = "exchangeable")
```

##

```{r, cache = F}
summary(cm_sleep)$coefficients
summary(mm_sleep_ind)$coefficients 
summary(mm_sleep_cs)$tTable
summary(mm_sleep_gee_cs)$coefficients 
summary(mm_sleep_gee_cs_alt)$coefficients
```

## An aside: `geeglm` assumes clusters are collected together

From `?geeglm`:

> Data are assumed to be sorted so that observations on each cluster appear 
as contiguous rows in data. If data is not sorted this way, the function 
will not identify the clusters correctly. If data is not sorted this way, 
a warning will be issued.

```{r}
set.seed(1)
sleepstudy %>% 
  # Randomly permuting rows
  slice_sample(n = nrow(.)) %>%
  # gives different result
  geeglm(Reaction ~ Days, 
         data = ., 
         id = Subject,
         corstr = "exchangeable")

```

Clusters are incorrectly identified with no warning. **Arrange your data by cluster**


## Was choice of constant correlation sensible?

```{r}
summary(mm_sleep_gee_cs)$corr
```

## Residuals from marginal model

```{r, echo = F}
ggplot(data = bind_cols(mm_sleep_gee_cs$data, 
                        Residuals = residuals(mm_sleep_gee_cs)[,1]),
       aes(x = Days, y = Residuals, group = Subject)) + 
  geom_line(alpha = 0.5) + 
  scale_x_continuous(breaks = 0:9, minor_breaks = NULL) + 
  theme(text = element_text(size = 22));
```

## Correlation plot from residuals 

### Suggests non-constant correlation

```{r, echo = F, fig.width = fig.x, fig.height =  1.3*fig.y}
library(ggcorrplot)
bind_cols(mm_sleep_gee_cs$data, 
          Residuals = residuals(mm_sleep_gee_cs)[,1]) %>% 
  as_tibble() %>%
  pivot_wider(id_cols = Subject, names_from = Days, values_from = Residuals, names_prefix = "Day") %>%
  select(-Subject) %>%
  as.matrix() %>%
  cor() %>%
  ggcorrplot(lab = TRUE, show.legend = FALSE)
```


## Possible modificiation 1: Marginal model with different correlation structure

```{r, cache = F}
# Compound-symmetric correlation
summary(mm_sleep_gee_cs)$coefficients

# Autoregressive-1 correlation
mm_sleep_gee_ar1 <- 
  geeglm(Reaction ~ Days, 
         data = sleepstudy, 
         id = Subject,
         corstr = "ar1")
summary(mm_sleep_gee_ar1)$coefficients
```


##  Possible modificiation 2: Expand conditional model with random slopes


```{r}
summary(cm_sleep)$coefficients
```

```{r}
cm_sleep_random_slope <- 
  lmer(Reaction ~ Days + (Days | Subject), sleepstudy);
summary(cm_sleep_random_slope)$coefficients
```

```{r}
extractAIC(cm_sleep)
extractAIC(cm_sleep_random_slope) # Better fit with random slope
```

## Can get subject-specific predictions

#### Best Linear Unbiased Predictions (BLUPs), $\hat{\mathbf{b}}_i$

```{r, echo = F}
ranef(cm_sleep_random_slope)
```


## Can get subject-specific predictions

#### $\mathbf{x}_i^\top \hat{\mathbf{\beta}}  + \mathbf{z}_i^\top\hat{\mathbf{b}}_i$ against Days


```{r, echo = F, fig.width=1.1*fig.x, fig.height=1.1*fig.y}
ggplot(data = bind_cols(sleepstudy, 
                        Prediction = predict(cm_sleep_random_slope, newdata = sleepstudy)),
       aes(x = Days, y = Prediction, group = Subject)) + 
  geom_line(alpha = 0.5) + 
  scale_x_continuous(breaks = 0:9, minor_breaks = NULL) + 
  theme(text = element_text(size = 22));
```


## Can get subject-specific predictions

#### $\mathbf{Y}$, $\mathbf{x}_i^\top \hat{\mathbf{\beta}}  + \mathbf{z}_i^\top\hat{\mathbf{b}}_i$ against Days

```{r, echo = F, fig.width=1.1*fig.x, fig.height=1.1*fig.y}
ggplot(data = bind_cols(sleepstudy, 
                        Prediction = predict(cm_sleep_random_slope, newdata = sleepstudy)) %>%
         filter(Subject %in% c("308", "309", "310", "330")),
       aes(x = Days, y = Prediction, group = Subject, color = Subject)) + 
  geom_line(size = 1) + 
  geom_line(aes(y = Reaction), size = 1, linetype = "dashed") + 
  scale_color_brewer(palette = "Dark2") + 
  guides(color = "none") + 
  scale_x_continuous(breaks = 0:9, minor_breaks = NULL) + 
  theme(text = element_text(size = 22));
```


## Summary: Point estimates

Point estimates of $\mathbf{\beta}$ generally stay same for conditional or marginal
linear models, but standard errors may differ

Robust / sandwich variance estimator is doing its job

## Summary: Interpretation

### Conditional model with random intercept and slope: 

$$
\begin{aligned}
E[Y_i|x = a] &= \beta_0 + b_{0i} + a (\beta + b_{1i})  \\
E[Y_i|x = a + 1] &= \beta_0 + b_{0i} + (a + 1) (\beta + b_{1i})\\
E[Y_i|x = a + 1] - E[Y_i|x = a] &= \beta + b_{1i}
\end{aligned}
$$
$\beta + b_{1i}$ is change in expected outcome per 1-unit increase in $x$
for an observation ingroup, and $\beta$ is change in expected outcome per 1-unit increase in $x$ when $b_{1i}=0$, i.e. when $b_{1i}$ is its population-average value


We are assuming group is held constant

## Summary: Interpretation

Interpretation of $\mathbf{\beta}$ similar for conditional or marginal models. With
one covariate:

### Marginal model

$$
\begin{aligned}
E[Y_i|x = a] &= \beta_0 +a \beta\\
E[Y_i|x = a + 1] &= \beta_0 + (a + 1) \beta\\
E[Y_i|x = a + 1] - E[Y_i|x = a] &= \beta 
\end{aligned}
$$
$\beta$ is change in expected outcome per 1-unit increase in $x$ 

We don't have to assume that group is held constant

## More differences for non-linear link functions

For non-linear link functions: 

- Point estimates are not same

- Interpretations may differ between conditional and marginal models, especially if you want to work on $E[Y|x]$ scale (as opposed to $g(E[Y|x])$ scale)

## References

\scriptsize

Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12

Højsgaard, S., Halekoh, U. & Yan J. (2006) The R
Package geepack for Generalized Estimating Equations
Journal of Statistical Software, 15, 2, pp1--11

Vincent J Carey. Ported to R by Thomas Lumley, Brian
Ripley. Files src/dgedi.f and src/dgefa.f are for
LINPACK authored by Cleve Moler. Note that
maintainers are not available to give advice on using
a package they did not author. (2019). gee:
Generalized Estimation Equation Solver. R package
version 4.13-20.
https://CRAN.R-project.org/package=gee

Pinheiro J, Bates D, DebRoy S, Sarkar D, R Core Team
(2019). _nlme: Linear and Nonlinear Mixed Effects
Models_. R package version 3.1-140, <URL:
https://CRAN.R-project.org/package=nlme>.

Douglas Bates, Martin Maechler, Ben Bolker, Steve
Walker (2015). Fitting Linear Mixed-Effects Models
Using lme4. Journal of Statistical Software, 67(1),
1-48. doi:10.18637/jss.v067.i01.

